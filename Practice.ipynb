{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920de79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09ebe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 95.0.4638\n",
      "Get LATEST chromedriver version for 95.0.4638 google-chrome\n",
      "Driver [/Users/liang/.wdm/drivers/chromedriver/mac64/95.0.4638.69/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfff5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **executable_path is unpacking the dictionary we've stored the path in â€“ think of it as unpacking a suitcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headless=False means that all of the browser's actions will be displayed in a Chrome window so we can see them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a513446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21af8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Quotes to Scrape site\n",
    "#This code tells Splinter which site we want to visit by assigning the link to a URL. \n",
    "#After executing the cell above, we will use BeautifulSoup to parse the HTML. \n",
    "#In the next cell, we'll add two more lines of code:\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c22dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML\n",
    "#Now we've parsed all of the HTML on the page. That means that BeautifulSoup has taken a look at the \n",
    "#different components and can now access them. Specifically, BeautifulSoup parses the HTML text and then \n",
    "#stores it as an object.\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff8c4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top Ten tags'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the Title\n",
    "#we will find the title and extract it\n",
    "title = html_soup.find('h2').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d22ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What we've just done in the last two lines of code is:\n",
    "\n",
    "#We used our html_soup object we created earlier and chained find() to it to search for the <h2 /> tag.\n",
    "#We've also extracted only the text within the HTML tags by adding .text to the end of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape All of the Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fd4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####QUESTION: how do we know that we need to create a variable and we cannot just go fin a and tags directly in the html.soup?\n",
    "\n",
    "# Scrape the top ten tags\n",
    "#The first line, tag_box = html_soup.find('div', class_='tags-box'), creates a new variable tag_box, \n",
    "#which will be used to store the results of a search. In this case, we're looking for <div /> elements with a \n",
    "#class of tags-box, and we're searching for it in the HTML we parsed earlier and stored in the html_soup variable.\n",
    "tag_box = html_soup.find('div', class_='tags-box')\n",
    "# tag_box\n",
    "#The second line, tags = tag_box.find_all('a', class_='tag'), is similar to the first but with a few tweaks to\n",
    "#make the search more specific. The new \"tags\" variable will hold the results of a find_all, but this time we're\n",
    "#searching through the parsed results stored in our tag_box variable to find <a /> elements with a tag class.\n",
    "tags = tag_box.find_all('a', class_='tag')\n",
    "\n",
    "#for loop. This for loop cycles through each tag in the tags variable, strips the HTML code out of it, \n",
    "#and then prints only the text of each tag\n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0ed671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Across Pages\n",
    "\n",
    "#We have already created the Browser instance and navigated to the http://quotes.toscrape.com/ page \n",
    "#with the visit() method. But, if you'd like to create the Browser instance again, run the following code in a\n",
    "#new cell.\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80cce91b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'descendants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3c09bcb334f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mquote_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m    \u001b[0mquotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'quote'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mquote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'page:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind_all\u001b[0;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResultSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \"\"\"\n\u001b[0;32m-> 1785\u001b[0;31m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescendants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'descendants'"
     ]
    }
   ],
   "source": [
    "#In the next cell, we'll create a for loop that will do the following:\n",
    "\n",
    "#Create a BeautifulSoup object\n",
    "#Find all the quotes on the page\n",
    "#Print each quote from the page\n",
    "#Click the \"Next\" button at the bottom of the page\n",
    "#We'll use range(1, 6) in our for loop to visit the first five pages of the website.\n",
    "for x in range(1, 6):\n",
    "   html = browser.html\n",
    "   quote_soup = soup(html, 'html.parser')\n",
    "   quotes = quote_soup.find_all('span', class_='text')\n",
    "   for quote in quotes:\n",
    "      print('page:', x, '----------')\n",
    "      print(quote.text)\n",
    "   browser.links.find_by_partial_text('Next').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77603ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
