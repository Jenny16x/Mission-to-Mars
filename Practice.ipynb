{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920de79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09ebe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 95.0.4638\n",
      "Get LATEST chromedriver version for 95.0.4638 google-chrome\n",
      "Driver [/Users/liang/.wdm/drivers/chromedriver/mac64/95.0.4638.69/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfff5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **executable_path is unpacking the dictionary we've stored the path in â€“ think of it as unpacking a suitcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headless=False means that all of the browser's actions will be displayed in a Chrome window so we can see them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a513446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21af8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Quotes to Scrape site\n",
    "#This code tells Splinter which site we want to visit by assigning the link to a URL. \n",
    "#After executing the cell above, we will use BeautifulSoup to parse the HTML. \n",
    "#In the next cell, we'll add two more lines of code:\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c22dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML\n",
    "#Now we've parsed all of the HTML on the page. That means that BeautifulSoup has taken a look at the \n",
    "#different components and can now access them. Specifically, BeautifulSoup parses the HTML text and then \n",
    "#stores it as an object.\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff8c4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top Ten tags'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the Title\n",
    "#we will find the title and extract it\n",
    "title = html_soup.find('h2').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d22ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What we've just done in the last two lines of code is:\n",
    "\n",
    "#We used our html_soup object we created earlier and chained find() to it to search for the <h2 /> tag.\n",
    "#We've also extracted only the text within the HTML tags by adding .text to the end of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape All of the Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fd4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####QUESTION: how do we know that we need to create a variable and we cannot just go fin a and tags directly in the html.soup?\n",
    "\n",
    "# Scrape the top ten tags\n",
    "#The first line, tag_box = html_soup.find('div', class_='tags-box'), creates a new variable tag_box, \n",
    "#which will be used to store the results of a search. In this case, we're looking for <div /> elements with a \n",
    "#class of tags-box, and we're searching for it in the HTML we parsed earlier and stored in the html_soup variable.\n",
    "tag_box = html_soup.find('div', class_='tags-box')\n",
    "# tag_box\n",
    "#The second line, tags = tag_box.find_all('a', class_='tag'), is similar to the first but with a few tweaks to\n",
    "#make the search more specific. The new \"tags\" variable will hold the results of a find_all, but this time we're\n",
    "#searching through the parsed results stored in our tag_box variable to find <a /> elements with a tag class.\n",
    "tags = tag_box.find_all('a', class_='tag')\n",
    "\n",
    "#for loop. This for loop cycles through each tag in the tags variable, strips the HTML code out of it, \n",
    "#and then prints only the text of each tag\n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0ed671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Across Pages\n",
    "\n",
    "#We have already created the Browser instance and navigated to the http://quotes.toscrape.com/ page \n",
    "#with the visit() method. But, if you'd like to create the Browser instance again, run the following code in a\n",
    "#new cell.\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80cce91b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'descendants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3c09bcb334f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0mquote_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m    \u001b[0mquotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'quote'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mquote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'page:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mfind_all\u001b[0;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResultSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \"\"\"\n\u001b[0;32m-> 1785\u001b[0;31m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescendants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'descendants'"
     ]
    }
   ],
   "source": [
    "#In the next cell, we'll create a for loop that will do the following:\n",
    "\n",
    "#Create a BeautifulSoup object\n",
    "#Find all the quotes on the page\n",
    "#Print each quote from the page\n",
    "#Click the \"Next\" button at the bottom of the page\n",
    "#We'll use range(1, 6) in our for loop to visit the first five pages of the website.\n",
    "for x in range(1, 6):\n",
    "   html = browser.html\n",
    "   quote_soup = soup(html, 'html.parser')\n",
    "   quotes = quote_soup.find_all('span', class_='text')\n",
    "   for quote in quotes:\n",
    "      print('page:', x, '----------')\n",
    "      print(quote.text)\n",
    "   browser.links.find_by_partial_text('Next').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb36a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#News Title and Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77603ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_news(browser):\n",
    "\n",
    "   # Visit the mars nasa news site\n",
    "   url = 'https://redplanetscience.com/'\n",
    "   browser.visit(url)\n",
    "\n",
    "   # Optional delay for loading the page\n",
    "   browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "   # Convert the browser html to a soup object and then quit the browser\n",
    "   html = browser.html\n",
    "   news_soup = soup(html, 'html.parser')\n",
    "\n",
    "   slide_elem = news_soup.select_one('div.list_text')\n",
    "\n",
    "   # Use the parent element to find the first <a> tag and save it as `news_title`\n",
    "   news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "\n",
    "   # Use the parent element to find the paragraph text\n",
    "   news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "   return news_title, news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126045b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of having our title and paragraph printed within the function, we want to return them from the function so\n",
    "#they can be used outside of it. We'll adjust our code to do so by deleting news_title and news_p and include them\n",
    "#in the return statement instead, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14834716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #When we add the word \"browser\" to our function, we're telling Python that we'll be using the browser variable we \n",
    "# #defined outside the function. All of our scraping code utilizes an automated browser, and without this section, \n",
    "# #our function wouldn't work.\n",
    "\n",
    "# The finishing touch is to add error handling to the mix. This is to address any potential errors that may occur \n",
    "#during web scraping. Errors can pop up from anywhere, but in web scraping the most common cause of an error is\n",
    "#when the webpage's format has changed and the scraping code no longer matches the new HTML elements.\n",
    "\n",
    "# We're going to add a try and except clause addressing AttributeErrors. By adding this error handling, \n",
    "#we are able to continue with our other scraping portions even if this one doesn't work.\n",
    "\n",
    "# In our code, we're going to add the try portion right before the scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding the try portion of our error handling, we need to add the except part. After these lines,\n",
    "# we'll immediately add the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84877cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By adding try: just before scraping, we're telling Python to look for these elements. If there's an error, \n",
    "#     Python will continue to run the remainder of the code. If it runs into an AttributeError, however, \n",
    "#     instead of returning the title and paragraph, Python will return nothing instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc24e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Featured Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934111c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featured_image(browser):\n",
    "    # Visit URL\n",
    "    url = 'https://spaceimages-mars.com'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "\n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Declare and define our function.\n",
    "\n",
    "# #Remove print statement(s) and return them instead.\n",
    "# #In our Jupyter Notebook version of the code, we printed the results of our scraping by simply stating the \n",
    "# variable (e.g., after assigning data to the img_url variable, we simply put img_url on the next line to view \n",
    "#           the data). We still want to view the data output in our Python script, but we want to see it at the \n",
    "# end of our function instead of within it.\n",
    "\n",
    "#Add error handling for AttributeError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mars Facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the facts table will be updated in a similar manner to the other two. This time, though, \n",
    "# we'll be adding BaseException to our except block for error handling.\n",
    "\n",
    "# A BaseException is a little bit of a catchall when it comes to error handling. It is raised when any \n",
    "# of the built-in exceptions are encountered and it won't handle any user-defined exceptions. \n",
    "# We're using it here because we're using Pandas' read_html() function to pull data, instead of\n",
    "# scraping with BeautifulSoup and Splinter. The data is returned a little differently and can result in errors\n",
    "# other than AttributeErrors, which is what we've been addressing so far.\n",
    "\n",
    "#As before, we've removed the print statements. Now that we know this code is working correctly, \n",
    "# we don't need to view the DataFrame that's generated.\n",
    "\n",
    "# The code to assign columns and set the index of the DataFrame will remain the same, so the last \n",
    "# update we need to complete for this function is to add the return statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e98234",
   "metadata": {},
   "outputs": [],
   "source": [
    "Integrate MongoDB Into the Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a962f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we make our website look pretty (you never know when NASA is looking for its new analyst), \n",
    "# we need to connect to Mongo and establish communication between our code and the database we're using. \n",
    "# We'll add this last bit of code to our scraping.py script.\n",
    "\n",
    "# At the top of our scraping.py script, just after importing the dependencies, we'll add one more function. \n",
    "# This function differs from the others in that it will:\n",
    "\n",
    "# Initialize the browser.\n",
    "# Create a data dictionary.\n",
    "# End the WebDriver and return the scraped data.\n",
    "# Let's define this function as \"scrape_all\" and then initiate the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41422de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all():\n",
    "    # Initiate headless driver for deployment\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we can see the word \"browser\" here twice, one is the name of the variable passed into the function \n",
    "# and the other is the name of a parameter. Coding guidelines do not require that these match, even though \n",
    "# they do in our current code.\n",
    "\n",
    "# When we were testing our code in Jupyter, headless was set as False so we could see the scraping in action. \n",
    "# Now that we are deploying our code into a usable web app, we don't need to watch the script work \n",
    "# (though it's totally okay if you still want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When scraping, the \"headless\" browsing session is when a browser is run without the users seeing it at all. \n",
    "# So, when headless=True is declared as we initiate the browser, we are telling it to run in headless mode. \n",
    "# All of the scraping will still be accomplished, but behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63efccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we're going to set our news title and paragraph variables (remember, this function will return two values).\n",
    "news_title, news_paragraph = mars_news(browser)\n",
    "#This line of code tells Python that we'll be using our mars_news function to pull this data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have our browser ready for work, we need to create the data dictionary. \n",
    "#Add the following code to our scrape_all() function:\n",
    "\n",
    "# Run all scraping functions and store results in dictionary\n",
    "data = {\n",
    "      \"news_title\": news_title,\n",
    "      \"news_paragraph\": news_paragraph,\n",
    "      \"featured_image\": featured_image(browser),\n",
    "      \"facts\": mars_facts(),\n",
    "      \"last_modified\": dt.datetime.now()\n",
    "}\n",
    "\n",
    "#This dictionary does two things: It runs all of the functions we've createdâ€”featured_image(browser), \n",
    "# for exampleâ€”and it also stores all of the results. When we create the HTML template, we'll create paths \n",
    "# to the dictionary's values, which lets us present our data on our template. We're also adding the date the \n",
    "# code was run last by adding \"last_modified\": dt.datetime.now(). For this line to work correctly, \n",
    "#     we'll also need to add import datetime as dt to our imported dependencies at the beginning of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b30330",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Stop webdriver and return data\n",
    "   browser.quit()\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730af7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To finish up the function, there are two more things to do. The first is to end the WebDriver using the \n",
    "# line browser.quit(). You can quit the automated browser by physically closing it, but there's a chance it won't \n",
    "# fully quit in the background. By using code to exit the browser, you'll know that all of the processes have been \n",
    "# stopped.\n",
    "\n",
    "# Second, the return statement needs to be added. This is the final line that will signal that the function is \n",
    "# complete, and it will be inserted directly beneath browser.quit(). We want to return the data dictionary created\n",
    "# earlier, so our return statement will simply read return data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf812a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This last block of code tells Flask that our script is complete and ready for action. \n",
    "# The print statement will print out the results of our scraping to our terminal after executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ddd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17465900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter, BeautifulSoup, and Pandas\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def scrape_all():\n",
    "    # Initiate headless driver for deployment\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "    news_title, news_paragraph = mars_news(browser)\n",
    "\n",
    "    # Run all scraping functions and store results in a dictionary\n",
    "    data = {\n",
    "        \"news_title\": news_title,\n",
    "        \"news_paragraph\": news_paragraph,\n",
    "        \"featured_image\": featured_image(browser),\n",
    "        \"facts\": mars_facts(),\n",
    "        \"last_modified\": dt.datetime.now()\n",
    "    }\n",
    "\n",
    "    # Stop webdriver and return data\n",
    "    browser.quit()\n",
    "    return data\n",
    "\n",
    "\n",
    "def mars_news(browser):\n",
    "\n",
    "    # Scrape Mars News\n",
    "    # Visit the mars nasa news site\n",
    "    url = 'https://data-class-mars.s3.amazonaws.com/Mars/index.html'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Optional delay for loading the page\n",
    "    browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "    # Convert the browser html to a soup object and then quit the browser\n",
    "    html = browser.html\n",
    "    news_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        slide_elem = news_soup.select_one('div.list_text')\n",
    "        # Use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "        # Use the parent element to find the paragraph text\n",
    "        news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "    except AttributeError:\n",
    "        return None, None\n",
    "\n",
    "    return news_title, news_p\n",
    "\n",
    "\n",
    "def featured_image(browser):\n",
    "    # Visit URL\n",
    "    url = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    img_url = f'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/{img_url_rel}'\n",
    "\n",
    "    return img_url\n",
    "\n",
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://data-class-mars-facts.s3.amazonaws.com/Mars_Facts/index.html')[0]\n",
    "\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html(classes=\"table table-striped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94965126",
   "metadata": {},
   "outputs": [],
   "source": [
    "note about bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's also a good idea at this point to run your code and check it for errors. Even though the Jupyter Notebook \n",
    "# cells have already been tested and bugs were addressed, because we made some slight updates and fine-tuned the \n",
    "# converted Python code, it's possible a new bug could have popped up.\n",
    "\n",
    "# NOTE\n",
    "# In your terminal, make sure you're in the correct directory with the ls command (if you don't see the files you've \n",
    "# been working on, then navigate to the folder you're storing them in). Make sure you have the correct environment \n",
    "# activated, then type python app.py into your terminal.\n",
    "\n",
    "# The next message you see on your terminal should be a message that the Flask application is running on localhost. \n",
    "# Enter that address (usually http://127.0.0.1:5000/) into the address bar of your web browser.\n",
    "\n",
    "# If you don't see that message on your terminal, you likely have a bug in your script. Thankfully, error messages \n",
    "# will help you pinpoint where and why an error is occurring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
