{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920de79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09ebe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 96.0.4664\n",
      "Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "Driver [/Users/liang/.wdm/drivers/chromedriver/mac64/96.0.4664.45/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfff5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **executable_path is unpacking the dictionary we've stored the path in – think of it as unpacking a suitcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638902aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headless=False means that all of the browser's actions will be displayed in a Chrome window so we can see them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a513446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21af8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Quotes to Scrape site\n",
    "#This code tells Splinter which site we want to visit by assigning the link to a URL. \n",
    "#After executing the cell above, we will use BeautifulSoup to parse the HTML. \n",
    "#In the next cell, we'll add two more lines of code:\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c22dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML\n",
    "#Now we've parsed all of the HTML on the page. That means that BeautifulSoup has taken a look at the \n",
    "#different components and can now access them. Specifically, BeautifulSoup parses the HTML text and then \n",
    "#stores it as an object.\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff8c4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top Ten tags'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the Title\n",
    "#we will find the title and extract it\n",
    "title = html_soup.find('h2').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d22ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What we've just done in the last two lines of code is:\n",
    "\n",
    "#We used our html_soup object we created earlier and chained find() to it to search for the <h2 /> tag.\n",
    "#We've also extracted only the text within the HTML tags by adding .text to the end of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729a5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape All of the Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2fd4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "inspirational\n",
      "life\n",
      "humor\n",
      "books\n",
      "reading\n",
      "friendship\n",
      "friends\n",
      "truth\n",
      "simile\n"
     ]
    }
   ],
   "source": [
    "#####QUESTION: how do we know that we need to create a variable and we cannot just go fin a and tags directly in the html.soup?\n",
    "\n",
    "# Scrape the top ten tags\n",
    "#The first line, tag_box = html_soup.find('div', class_='tags-box'), creates a new variable tag_box, \n",
    "#which will be used to store the results of a search. In this case, we're looking for <div /> elements with a \n",
    "#class of tags-box, and we're searching for it in the HTML we parsed earlier and stored in the html_soup variable.\n",
    "tag_box = html_soup.find('div', class_='tags-box')\n",
    "# tag_box\n",
    "#The second line, tags = tag_box.find_all('a', class_='tag'), is similar to the first but with a few tweaks to\n",
    "#make the search more specific. The new \"tags\" variable will hold the results of a find_all, but this time we're\n",
    "#searching through the parsed results stored in our tag_box variable to find <a /> elements with a tag class.\n",
    "tags = tag_box.find_all('a', class_='tag')\n",
    "\n",
    "#for loop. This for loop cycles through each tag in the tags variable, strips the HTML code out of it, \n",
    "#and then prints only the text of each tag\n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0ed671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Across Pages\n",
    "\n",
    "#We have already created the Browser instance and navigated to the http://quotes.toscrape.com/ page \n",
    "#with the visit() method. But, if you'd like to create the Browser instance again, run the following code in a\n",
    "#new cell.\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cce91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 1 ----------\n",
      "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "page: 1 ----------\n",
      "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "page: 1 ----------\n",
      "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "page: 1 ----------\n",
      "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "page: 1 ----------\n",
      "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
      "page: 1 ----------\n",
      "“Try not to become a man of success. Rather become a man of value.”\n",
      "page: 1 ----------\n",
      "“It is better to be hated for what you are than to be loved for what you are not.”\n",
      "page: 1 ----------\n",
      "“I have not failed. I've just found 10,000 ways that won't work.”\n",
      "page: 1 ----------\n",
      "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
      "page: 1 ----------\n",
      "“A day without sunshine is like, you know, night.”\n",
      "page: 2 ----------\n",
      "“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\n",
      "page: 2 ----------\n",
      "“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”\n",
      "page: 2 ----------\n",
      "“If you can't explain it to a six year old, you don't understand it yourself.”\n",
      "page: 2 ----------\n",
      "“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\n",
      "page: 2 ----------\n",
      "“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”\n",
      "page: 2 ----------\n",
      "“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”\n",
      "page: 2 ----------\n",
      "“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\n",
      "page: 2 ----------\n",
      "“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”\n",
      "page: 2 ----------\n",
      "“Good friends, good books, and a sleepy conscience: this is the ideal life.”\n",
      "page: 2 ----------\n",
      "“Life is what happens to us while we are making other plans.”\n",
      "page: 3 ----------\n",
      "“I love you without knowing how, or when, or from where. I love you simply, without problems or pride: I love you in this way because I do not know any other way of loving but this, in which there is no I or you, so intimate that your hand upon my chest is my hand, so intimate that when I fall asleep your eyes close.”\n",
      "page: 3 ----------\n",
      "“For every minute you are angry you lose sixty seconds of happiness.”\n",
      "page: 3 ----------\n",
      "“If you judge people, you have no time to love them.”\n",
      "page: 3 ----------\n",
      "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
      "page: 3 ----------\n",
      "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
      "page: 3 ----------\n",
      "“Today you are You, that is truer than true. There is no one alive who is Youer than You.”\n",
      "page: 3 ----------\n",
      "“If you want your children to be intelligent, read them fairy tales. If you want them to be more intelligent, read them more fairy tales.”\n",
      "page: 3 ----------\n",
      "“It is impossible to live without failing at something, unless you live so cautiously that you might as well not have lived at all - in which case, you fail by default.”\n",
      "page: 3 ----------\n",
      "“Logic will get you from A to Z; imagination will get you everywhere.”\n",
      "page: 3 ----------\n",
      "“One good thing about music, when it hits you, you feel no pain.”\n",
      "page: 4 ----------\n",
      "“The more that you read, the more things you will know. The more that you learn, the more places you'll go.”\n",
      "page: 4 ----------\n",
      "“Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?”\n",
      "page: 4 ----------\n",
      "“The truth is, everyone is going to hurt you. You just got to find the ones worth suffering for.”\n",
      "page: 4 ----------\n",
      "“Not all of us can do great things. But we can do small things with great love.”\n",
      "page: 4 ----------\n",
      "“To the well-organized mind, death is but the next great adventure.”\n",
      "page: 4 ----------\n",
      "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
      "page: 4 ----------\n",
      "“We read to know we're not alone.”\n",
      "page: 4 ----------\n",
      "“Any fool can know. The point is to understand.”\n",
      "page: 4 ----------\n",
      "“I have always imagined that Paradise will be a kind of library.”\n",
      "page: 4 ----------\n",
      "“It is never too late to be what you might have been.”\n",
      "page: 5 ----------\n",
      "“A reader lives a thousand lives before he dies, said Jojen. The man who never reads lives only one.”\n",
      "page: 5 ----------\n",
      "“You can never get a cup of tea large enough or a book long enough to suit me.”\n",
      "page: 5 ----------\n",
      "“You believe lies so you eventually learn to trust no one but yourself.”\n",
      "page: 5 ----------\n",
      "“If you can make a woman laugh, you can make her do anything.”\n",
      "page: 5 ----------\n",
      "“Life is like riding a bicycle. To keep your balance, you must keep moving.”\n",
      "page: 5 ----------\n",
      "“The real lover is the man who can thrill you by kissing your forehead or smiling into your eyes or just staring into space.”\n",
      "page: 5 ----------\n",
      "“A wise girl kisses but doesn't love, listens but doesn't believe, and leaves before she is left.”\n",
      "page: 5 ----------\n",
      "“Only in the darkness can you see the stars.”\n",
      "page: 5 ----------\n",
      "“It matters not what someone is born, but what they grow to be.”\n",
      "page: 5 ----------\n",
      "“Love does not begin and end the way we seem to think it does. Love is a battle, love is a war; love is a growing up.”\n"
     ]
    }
   ],
   "source": [
    "#In the next cell, we'll create a for loop that will do the following:\n",
    "\n",
    "#Create a BeautifulSoup object\n",
    "#Find all the quotes on the page\n",
    "#Print each quote from the page\n",
    "#Click the \"Next\" button at the bottom of the page\n",
    "#We'll use range(1, 6) in our for loop to visit the first five pages of the website.\n",
    "for x in range(1, 6):\n",
    "   html = browser.html\n",
    "   quote_soup = soup(html, 'html.parser')\n",
    "   quotes = quote_soup.find_all('span', class_='text')\n",
    "   for quote in quotes:\n",
    "      print('page:', x, '----------')\n",
    "      print(quote.text)\n",
    "   browser.links.find_by_partial_text('Next').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcbe2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#News Title and Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77603ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_news(browser):\n",
    "\n",
    "   # Visit the mars nasa news site\n",
    "   url = 'https://redplanetscience.com/'\n",
    "   browser.visit(url)\n",
    "\n",
    "   # Optional delay for loading the page\n",
    "   browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "   # Convert the browser html to a soup object and then quit the browser\n",
    "   html = browser.html\n",
    "   news_soup = soup(html, 'html.parser')\n",
    "\n",
    "   slide_elem = news_soup.select_one('div.list_text')\n",
    "\n",
    "   # Use the parent element to find the first <a> tag and save it as `news_title`\n",
    "   news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "\n",
    "   # Use the parent element to find the paragraph text\n",
    "   news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "   return news_title, news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abbb5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of having our title and paragraph printed within the function, we want to return them from the function so\n",
    "#they can be used outside of it. We'll adjust our code to do so by deleting news_title and news_p and include them\n",
    "#in the return statement instead, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "933b3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #When we add the word \"browser\" to our function, we're telling Python that we'll be using the browser variable we \n",
    "# #defined outside the function. All of our scraping code utilizes an automated browser, and without this section, \n",
    "# #our function wouldn't work.\n",
    "\n",
    "# The finishing touch is to add error handling to the mix. This is to address any potential errors that may occur \n",
    "#during web scraping. Errors can pop up from anywhere, but in web scraping the most common cause of an error is\n",
    "#when the webpage's format has changed and the scraping code no longer matches the new HTML elements.\n",
    "\n",
    "# We're going to add a try and except clause addressing AttributeErrors. By adding this error handling, \n",
    "#we are able to continue with our other scraping portions even if this one doesn't work.\n",
    "\n",
    "# In our code, we're going to add the try portion right before the scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cc9bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding the try portion of our error handling, we need to add the except part. After these lines,\n",
    "# we'll immediately add the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d38ef90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By adding try: just before scraping, we're telling Python to look for these elements. If there's an error, \n",
    "#     Python will continue to run the remainder of the code. If it runs into an AttributeError, however, \n",
    "#     instead of returning the title and paragraph, Python will return nothing instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7589e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Featured Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2044ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featured_image(browser):\n",
    "    # Visit URL\n",
    "    url = 'https://spaceimages-mars.com'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "\n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edc91810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Declare and define our function.\n",
    "\n",
    "# #Remove print statement(s) and return them instead.\n",
    "# #In our Jupyter Notebook version of the code, we printed the results of our scraping by simply stating the \n",
    "# variable (e.g., after assigning data to the img_url variable, we simply put img_url on the next line to view \n",
    "#           the data). We still want to view the data output in our Python script, but we want to see it at the \n",
    "# end of our function instead of within it.\n",
    "\n",
    "#Add error handling for AttributeError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5dff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mars Facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201900a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876023a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the facts table will be updated in a similar manner to the other two. This time, though, \n",
    "# we'll be adding BaseException to our except block for error handling.\n",
    "\n",
    "# A BaseException is a little bit of a catchall when it comes to error handling. It is raised when any \n",
    "# of the built-in exceptions are encountered and it won't handle any user-defined exceptions. \n",
    "# We're using it here because we're using Pandas' read_html() function to pull data, instead of\n",
    "# scraping with BeautifulSoup and Splinter. The data is returned a little differently and can result in errors\n",
    "# other than AttributeErrors, which is what we've been addressing so far.\n",
    "\n",
    "#As before, we've removed the print statements. Now that we know this code is working correctly, \n",
    "# we don't need to view the DataFrame that's generated.\n",
    "\n",
    "# The code to assign columns and set the index of the DataFrame will remain the same, so the last \n",
    "# update we need to complete for this function is to add the return statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aaabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Integrate MongoDB Into the Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5db20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we make our website look pretty (you never know when NASA is looking for its new analyst), \n",
    "# we need to connect to Mongo and establish communication between our code and the database we're using. \n",
    "# We'll add this last bit of code to our scraping.py script.\n",
    "\n",
    "# At the top of our scraping.py script, just after importing the dependencies, we'll add one more function. \n",
    "# This function differs from the others in that it will:\n",
    "\n",
    "# Initialize the browser.\n",
    "# Create a data dictionary.\n",
    "# End the WebDriver and return the scraped data.\n",
    "# Let's define this function as \"scrape_all\" and then initiate the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all():\n",
    "    # Initiate headless driver for deployment\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013adf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we can see the word \"browser\" here twice, one is the name of the variable passed into the function \n",
    "# and the other is the name of a parameter. Coding guidelines do not require that these match, even though \n",
    "# they do in our current code.\n",
    "\n",
    "# When we were testing our code in Jupyter, headless was set as False so we could see the scraping in action. \n",
    "# Now that we are deploying our code into a usable web app, we don't need to watch the script work \n",
    "# (though it's totally okay if you still want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98396888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When scraping, the \"headless\" browsing session is when a browser is run without the users seeing it at all. \n",
    "# So, when headless=True is declared as we initiate the browser, we are telling it to run in headless mode. \n",
    "# All of the scraping will still be accomplished, but behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we're going to set our news title and paragraph variables (remember, this function will return two values).\n",
    "news_title, news_paragraph = mars_news(browser)\n",
    "#This line of code tells Python that we'll be using our mars_news function to pull this data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have our browser ready for work, we need to create the data dictionary. \n",
    "#Add the following code to our scrape_all() function:\n",
    "\n",
    "# Run all scraping functions and store results in dictionary\n",
    "data = {\n",
    "      \"news_title\": news_title,\n",
    "      \"news_paragraph\": news_paragraph,\n",
    "      \"featured_image\": featured_image(browser),\n",
    "      \"facts\": mars_facts(),\n",
    "      \"last_modified\": dt.datetime.now()\n",
    "}\n",
    "\n",
    "#This dictionary does two things: It runs all of the functions we've created—featured_image(browser), \n",
    "# for example—and it also stores all of the results. When we create the HTML template, we'll create paths \n",
    "# to the dictionary's values, which lets us present our data on our template. We're also adding the date the \n",
    "# code was run last by adding \"last_modified\": dt.datetime.now(). For this line to work correctly, \n",
    "#     we'll also need to add import datetime as dt to our imported dependencies at the beginning of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Stop webdriver and return data\n",
    "   browser.quit()\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To finish up the function, there are two more things to do. The first is to end the WebDriver using the \n",
    "# line browser.quit(). You can quit the automated browser by physically closing it, but there's a chance it won't \n",
    "# fully quit in the background. By using code to exit the browser, you'll know that all of the processes have been \n",
    "# stopped.\n",
    "\n",
    "# Second, the return statement needs to be added. This is the final line that will signal that the function is \n",
    "# complete, and it will be inserted directly beneath browser.quit(). We want to return the data dictionary created\n",
    "# earlier, so our return statement will simply read return data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904767fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This last block of code tells Flask that our script is complete and ready for action. \n",
    "# The print statement will print out the results of our scraping to our terminal after executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8973e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter, BeautifulSoup, and Pandas\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def scrape_all():\n",
    "    # Initiate headless driver for deployment\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "    news_title, news_paragraph = mars_news(browser)\n",
    "\n",
    "    # Run all scraping functions and store results in a dictionary\n",
    "    data = {\n",
    "        \"news_title\": news_title,\n",
    "        \"news_paragraph\": news_paragraph,\n",
    "        \"featured_image\": featured_image(browser),\n",
    "        \"facts\": mars_facts(),\n",
    "        \"last_modified\": dt.datetime.now()\n",
    "    }\n",
    "\n",
    "    # Stop webdriver and return data\n",
    "    browser.quit()\n",
    "    return data\n",
    "\n",
    "\n",
    "def mars_news(browser):\n",
    "\n",
    "    # Scrape Mars News\n",
    "    # Visit the mars nasa news site\n",
    "    url = 'https://data-class-mars.s3.amazonaws.com/Mars/index.html'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Optional delay for loading the page\n",
    "    browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "    # Convert the browser html to a soup object and then quit the browser\n",
    "    html = browser.html\n",
    "    news_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        slide_elem = news_soup.select_one('div.list_text')\n",
    "        # Use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "        # Use the parent element to find the paragraph text\n",
    "        news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "    except AttributeError:\n",
    "        return None, None\n",
    "\n",
    "    return news_title, news_p\n",
    "\n",
    "\n",
    "def featured_image(browser):\n",
    "    # Visit URL\n",
    "    url = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    img_url = f'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/{img_url_rel}'\n",
    "\n",
    "    return img_url\n",
    "\n",
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://data-class-mars-facts.s3.amazonaws.com/Mars_Facts/index.html')[0]\n",
    "\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html(classes=\"table table-striped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "note about bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800116f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's also a good idea at this point to run your code and check it for errors. Even though the Jupyter Notebook \n",
    "# cells have already been tested and bugs were addressed, because we made some slight updates and fine-tuned the \n",
    "# converted Python code, it's possible a new bug could have popped up.\n",
    "\n",
    "# NOTE\n",
    "# In your terminal, make sure you're in the correct directory with the ls command (if you don't see the files you've \n",
    "# been working on, then navigate to the folder you're storing them in). Make sure you have the correct environment \n",
    "# activated, then type python app.py into your terminal.\n",
    "\n",
    "# The next message you see on your terminal should be a message that the Flask application is running on localhost. \n",
    "# Enter that address (usually http://127.0.0.1:5000/) into the address bar of your web browser.\n",
    "\n",
    "# If you don't see that message on your terminal, you likely have a bug in your script. Thankfully, error messages \n",
    "# will help you pinpoint where and why an error is occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ec09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
